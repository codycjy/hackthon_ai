# filter_engine/ml_classifier.py
from typing import Dict, List, Optional
from dataclasses import dataclass
import logging
import time
import numpy as np
from config.settings import CommentCategory

from detoxify import Detoxify

logger = logging.getLogger(__name__)

@dataclass
class ClassificationResult:
    """åˆ†ç±»ç»“æœ"""
    category: CommentCategory
    confidence: float
    all_scores: Dict[CommentCategory, float]
    detoxify_raw: Dict[str, float]  # åŸå§‹Detoxifyè¾“å‡º


class LightweightClassifier:
    """
    åŸºäºDetoxifyçš„MLåˆ†ç±»å™¨

    
    è¾“å‡ºæ ‡ç­¾:
    - toxicity: æ€»ä½“æ¯’æ€§
    - severe_toxicity: ä¸¥é‡æ¯’æ€§
    - obscene: æ·«ç§½
    - threat: å¨èƒ
    - insult: ä¾®è¾±
    - identity_attack: èº«ä»½æ”»å‡»
    - sexual_explicit: è‰²æƒ…å†…å®¹
    """
    
    def __init__(self, model_type: str = 'multilingual', device: str = 'cpu'):
        """
        åˆå§‹åŒ–Detoxifyæ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('original', 'unbiased', 'multilingual')
            device: è¿è¡Œè®¾å¤‡ ('cpu', 'cuda')
        """
        self.model_type = model_type
        self.device = device
        
        # åŠ è½½Detoxifyæ¨¡å‹
        logger.info(f"Loading Detoxify model: {model_type} on {device}...")
        load_start = time.time()
        self.model = Detoxify(model_type, device=device)
        logger.info(f"Detoxify model loaded in {time.time() - load_start:.2f}s")
        
        # Detoxifyæ ‡ç­¾åˆ°è‡ªå®šä¹‰ç±»åˆ«çš„æ˜ å°„
        self.label_mapping = {
            'threat': CommentCategory.THREAT,
            'identity_attack': CommentCategory.HATE_IDENTITY,
            'insult': CommentCategory.HATE_APPEARANCE,  # ä¾®è¾±æ˜ å°„åˆ°å¤–è²Œæ”»å‡»ï¼ˆå¯è°ƒæ•´ï¼‰
            'severe_toxicity': CommentCategory.TOXIC,
            'toxicity': CommentCategory.TOXIC,
            'obscene': CommentCategory.TOXIC,
        }
        
        # å„ç±»åˆ«çš„é˜ˆå€¼é…ç½®
        self.thresholds = {
            'threat': 0.5,
            'identity_attack': 0.5,
            'severe_toxicity': 0.7,
            'insult': 0.6,
            'toxicity': 0.7,
            'obscene': 0.7,
        }
    
    def classify(self, text: str, features: Dict) -> ClassificationResult:
        """
        ä½¿ç”¨Detoxifyåˆ†ç±»è¯„è®º

        Args:
            text: å¾…åˆ†ç±»æ–‡æœ¬
            features: é¢„å¤„ç†æå–çš„ç‰¹å¾ï¼ˆç”¨äºè¾…åŠ©åˆ¤æ–­ï¼‰

        Returns:
            ClassificationResult: åˆ†ç±»ç»“æœ
        """
        start_time = time.time()
        logger.debug(f"ML classify: text={text[:60]}{'...' if len(text) > 60 else ''}")

        # è·å–Detoxifyé¢„æµ‹ç»“æœ
        detoxify_results = self.model.predict(text)
        predict_elapsed = time.time() - start_time
        logger.debug(f"Detoxify predict in {predict_elapsed:.3f}s | toxicity={detoxify_results.get('toxicity', 0):.4f} severe={detoxify_results.get('severe_toxicity', 0):.4f} threat={detoxify_results.get('threat', 0):.4f} insult={detoxify_results.get('insult', 0):.4f} identity_attack={detoxify_results.get('identity_attack', 0):.4f}")

        # è½¬æ¢ä¸ºè‡ªå®šä¹‰ç±»åˆ«å¾—åˆ†
        category_scores = self._convert_to_category_scores(detoxify_results, features)

        # æ‰¾åˆ°æœ€é«˜åˆ†ç±»åˆ«
        best_category = max(category_scores, key=category_scores.get)
        best_score = category_scores[best_category]

        # è®¡ç®—å½’ä¸€åŒ–ç½®ä¿¡åº¦
        total_score = sum(category_scores.values())
        if total_score > 0:
            confidence = best_score / total_score
        else:
            confidence = 0.0

        # åº”ç”¨è±å…é€»è¾‘é™ä½ç½®ä¿¡åº¦
        if features.get("exemption_matches"):
            if best_category != CommentCategory.THREAT:
                original_confidence = confidence
                confidence *= 0.6
                logger.debug(f"Exemption applied: confidence {original_confidence:.2f} -> {confidence:.2f}")
                # å¦‚æœæœ‰è±å…åŒ¹é…ä¸”ç½®ä¿¡åº¦é™ä½åè¾ƒä½ï¼Œè€ƒè™‘åˆ¤ä¸ºSAFE
                if confidence < 0.4:
                    logger.debug(f"Low confidence after exemption, overriding category {best_category.value} -> safe")
                    best_category = CommentCategory.SAFE
                    best_score = 1 - detoxify_results.get('toxicity', 0)

        final_confidence = min(confidence * 1.3, 0.99)
        elapsed = time.time() - start_time
        logger.info(f"ML classify done in {elapsed:.3f}s | category={best_category.value} confidence={final_confidence:.2f}")

        return ClassificationResult(
            category=best_category,
            confidence=final_confidence,
            all_scores=category_scores,
            detoxify_raw=detoxify_results
        )
    
    def _convert_to_category_scores(
        self, 
        detoxify_results: Dict[str, float],
        features: Dict
    ) -> Dict[CommentCategory, float]:
        """
        å°†Detoxifyè¾“å‡ºè½¬æ¢ä¸ºè‡ªå®šä¹‰ç±»åˆ«å¾—åˆ†
        
        Args:
            detoxify_results: DetoxifyåŸå§‹è¾“å‡º
            features: é¢„å¤„ç†ç‰¹å¾
        
        Returns:
            å„ç±»åˆ«çš„å¾—åˆ†å­—å…¸
        """
        scores = {cat: 0.0 for cat in CommentCategory}
        
        # 1. å¨èƒç±» (THREAT)
        threat_score = detoxify_results.get('threat', 0)
        if threat_score > self.thresholds['threat']:
            scores[CommentCategory.THREAT] = threat_score
        
        # 2. èº«ä»½æ”»å‡» (HATE_IDENTITY)
        identity_score = detoxify_results.get('identity_attack', 0)
        if identity_score > self.thresholds['identity_attack']:
            scores[CommentCategory.HATE_IDENTITY] = identity_score
        
        # 3. å¤–è²Œæ”»å‡» (HATE_APPEARANCE) - ç»“åˆinsultå’Œå…³é”®è¯
        insult_score = detoxify_results.get('insult', 0)
        keyword_matches = features.get("keyword_matches", {})
        if "hate_appearance" in keyword_matches:
            scores[CommentCategory.HATE_APPEARANCE] = max(insult_score, 0.7)
        elif insult_score > self.thresholds['insult']:
            scores[CommentCategory.HATE_APPEARANCE] = insult_score * 0.8
        
        # 4. é€ è°£ (DISTORTION) - Detoxifyä¸ç›´æ¥æ”¯æŒï¼Œä¾èµ–å…³é”®è¯
        if "distortion" in keyword_matches:
            scores[CommentCategory.DISTORTION] = 0.7 + len(keyword_matches.get("distortion", [])) * 0.1
        
        # 5. æ¶æ„è¯„è®º (TOXIC) - ç»¼åˆtoxicityå’Œsevere_toxicity
        toxicity = detoxify_results.get('toxicity', 0)
        severe_toxicity = detoxify_results.get('severe_toxicity', 0)
        obscene = detoxify_results.get('obscene', 0)
        
        toxic_score = max(toxicity, severe_toxicity, obscene)
        if toxic_score > self.thresholds['toxicity']:
            # é¿å…ä¸å…¶ä»–æ›´å…·ä½“çš„ç±»åˆ«é‡å¤è®¡åˆ†
            if scores[CommentCategory.THREAT] < 0.5 and scores[CommentCategory.HATE_IDENTITY] < 0.5:
                scores[CommentCategory.TOXIC] = toxic_score
        
        # 6. å¼•æµ (TRAFFIC_HIJACKING) - Detoxifyä¸æ”¯æŒï¼Œå®Œå…¨ä¾èµ–å…³é”®è¯
        if "traffic_hijacking" in keyword_matches:
            scores[CommentCategory.TRAFFIC_HIJACKING] = 0.75 + len(keyword_matches.get("traffic_hijacking", [])) * 0.1
        if features.get("has_url") and features.get("mention_count", 0) > 0:
            scores[CommentCategory.TRAFFIC_HIJACKING] = max(
                scores[CommentCategory.TRAFFIC_HIJACKING], 
                0.5
            )
        
        # 7. è¯ˆéª— (SCAM_SPAM) - Detoxifyä¸æ”¯æŒï¼Œä¾èµ–å…³é”®è¯
        if "scam_spam" in keyword_matches:
            scores[CommentCategory.SCAM_SPAM] = 0.8 + len(keyword_matches.get("scam_spam", [])) * 0.1
        
        # 8. å®‰å…¨/è±å… (SAFE)
        exemption_matches = features.get("exemption_matches", [])
        if exemption_matches:
            # æœ‰è±å…æ¨¡å¼åŒ¹é…æ—¶ï¼Œæé«˜SAFEå¾—åˆ†
            safe_boost = len(exemption_matches) * 0.3
            non_toxic_score = 1 - toxicity
            scores[CommentCategory.SAFE] = min(non_toxic_score + safe_boost, 1.0)
        else:
            # ä½æ¯’æ€§æ–‡æœ¬
            if toxicity < 0.3:
                scores[CommentCategory.SAFE] = 1 - toxicity
        
        # æ¶æ„emojiåŠ æˆ
        toxic_emoji_count = features.get("toxic_emoji_count", 0)
        if toxic_emoji_count > 0:
            scores[CommentCategory.TOXIC] = max(
                scores[CommentCategory.TOXIC],
                0.5 + toxic_emoji_count * 0.15
            )
        
        return scores
    
    def predict_batch(self, texts: List[str], features_list: List[Dict]) -> List[ClassificationResult]:
        """
        æ‰¹é‡åˆ†ç±»
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            features_list: å¯¹åº”çš„ç‰¹å¾åˆ—è¡¨
        
        Returns:
            åˆ†ç±»ç»“æœåˆ—è¡¨
        """
        # Detoxifyæ”¯æŒæ‰¹é‡é¢„æµ‹
        detoxify_results_batch = self.model.predict(texts)
        
        results = []
        for i, text in enumerate(texts):
            # å°†æ‰¹é‡ç»“æœè½¬æ¢ä¸ºå•æ¡æ ¼å¼
            single_result = {
                key: values[i] if isinstance(values, list) else values
                for key, values in detoxify_results_batch.items()
            }
            
            features = features_list[i] if i < len(features_list) else {}
            category_scores = self._convert_to_category_scores(single_result, features)
            
            best_category = max(category_scores, key=category_scores.get)
            best_score = category_scores[best_category]
            total_score = sum(category_scores.values())
            confidence = best_score / total_score if total_score > 0 else 0.0
            
            if features.get("exemption_matches") and best_category != CommentCategory.THREAT:
                confidence *= 0.6
                if confidence < 0.4:
                    best_category = CommentCategory.SAFE
            
            results.append(ClassificationResult(
                category=best_category,
                confidence=min(confidence * 1.3, 0.99),
                all_scores=category_scores,
                detoxify_raw=single_result
            ))
        
        return results
    
    def get_toxicity_scores(self, text: str) -> Dict[str, float]:
        """
        è·å–åŸå§‹Detoxifyæ¯’æ€§åˆ†æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
        
        Returns:
            Detoxifyå„æ ‡ç­¾çš„åˆ†æ•°
        """
        return self.model.predict(text)


# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºå¸¦æœ‰GPUæ”¯æŒçš„åˆ†ç±»å™¨
def create_classifier(use_gpu: bool = False, model_type: str = 'multilingual') -> LightweightClassifier:
    """
    åˆ›å»ºåˆ†ç±»å™¨å®ä¾‹
    
    Args:
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
        model_type: æ¨¡å‹ç±»å‹
    
    Returns:
        é…ç½®å¥½çš„åˆ†ç±»å™¨
    """
    device = 'cuda' if use_gpu else 'cpu'
    return LightweightClassifier(model_type=model_type, device=device)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = LightweightClassifier(model_type='multilingual')
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "text": "å»æ­»å§ä½ è¿™ä¸ªåƒåœ¾",
            "features": {"keyword_matches": {"threat": ["å»æ­»"], "toxic": ["åƒåœ¾"]}, "exemption_matches": [], "toxic_emoji_count": 0, "has_url": False, "mention_count": 0}
        },
        {
            "text": "You should kill yourself, nobody likes you",
            "features": {"keyword_matches": {"threat": ["kill yourself"]}, "exemption_matches": [], "toxic_emoji_count": 0, "has_url": False, "mention_count": 0}
        },
        {
            "text": "å¤ªç¾äº†å§ï¼æˆ‘æ¨ä½ æ€ä¹ˆè¿™ä¹ˆå¥½çœ‹",
            "features": {"keyword_matches": {}, "exemption_matches": ["å¤ªç¾äº†", "å¥½çœ‹"], "toxic_emoji_count": 0, "has_url": False, "mention_count": 0}
        },
        {
            "text": "OMG slay queen! You look absolutely stunning ğŸ’•",
            "features": {"keyword_matches": {}, "exemption_matches": ["slay", "queen"], "toxic_emoji_count": 0, "has_url": False, "mention_count": 0}
        },
        {
            "text": "è¿™èº«æä¹Ÿå¥½æ„æ€å‘ï¼Ÿåƒå¤´çŒªä¸€æ ·",
            "features": {"keyword_matches": {"hate_appearance": ["çŒª"]}, "exemption_matches": [], "toxic_emoji_count": 0, "has_url": False, "mention_count": 0}
        },
        {
            "text": "æŠ•èµ„æ¯”ç‰¹å¸ç¨³èµšä¸èµ”ï¼ŒåŠ æˆ‘å¾®ä¿¡äº†è§£",
            "features": {"keyword_matches": {"scam_spam": ["æ¯”ç‰¹å¸", "åŠ æˆ‘å¾®ä¿¡"]}, "exemption_matches": [], "toxic_emoji_count": 0, "has_url": False, "mention_count": 0}
        },
        {
            "text": "ğŸ’©ğŸ’©ğŸ’© åƒåœ¾å†…å®¹ ğŸ¤®ğŸ¤®ğŸ¤®",
            "features": {"keyword_matches": {"toxic": ["åƒåœ¾"]}, "exemption_matches": [], "toxic_emoji_count": 6, "has_url": False, "mention_count": 0}
        },
    ]
    
    print("=" * 70)
    print("Detoxify MLåˆ†ç±»å™¨æµ‹è¯•")
    print("=" * 70)
    
    for i, case in enumerate(test_cases, 1):
        result = classifier.classify(case["text"], case["features"])
        print(f"\n{i}. æ–‡æœ¬: {case['text'][:50]}...")
        print(f"   åˆ†ç±»ç»“æœ: {result.category.value}")
        print(f"   ç½®ä¿¡åº¦: {result.confidence:.2%}")
        print(f"   DetoxifyåŸå§‹åˆ†æ•°: ", end="")
        formatted_raw = {k: f"{v:.4f}" for k, v in result.detoxify_raw.items()}
        print(formatted_raw)
        # åªæ˜¾ç¤ºåˆ†æ•°>0.1çš„æ ‡ç­¾ï¼Œä¿ç•™3ä½å°æ•°
        for label, score in result.detoxify_raw.items():
            if score > 0.1:
                print(f"      â†’ {label}: {score:.3f}")